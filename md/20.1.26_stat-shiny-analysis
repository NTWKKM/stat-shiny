# ðŸ“Š Stat-Shiny Repository Comprehensive Analysis & Enhancement Recommendations

**Report Date:** January 20, 2026  
**Branch Analyzed:** `patch`  
**Objective:** Review current architecture and provide strategic recommendations for adding professional medical statistics modules suitable for world-class publications

---

## Executive Summary

**stat-shiny** is a well-architected, modular Python Shiny application for medical statistical analysis. Currently featuring 7 main analysis modules, the platform demonstrates professional-grade design with proper logging, configuration management, and comprehensive missing data handling.

### Current Strengths
âœ… **Modular Tab Architecture** - 7 independent but interconnected analytical modules  
âœ… **Advanced Statistical Methods** - Firth's regression, Cox models, PSM, survival analysis  
âœ… **Publication-Ready Output** - Forest plots, detailed HTML reports, forest plot generation  
âœ… **Missing Data Framework** - Transparent reporting aligned with best practices  
âœ… **Enterprise Infrastructure** - Docker-ready, logging, configuration management  
âœ… **Code Quality** - Type hints, docstrings, error handling  

### Strategic Gaps (for Publication Excellence)
âš ï¸ **Effect Size Standardization** - Missing procedures for comparing across studies  
âš ï¸ **Publication Bias Assessment** - No funnel plots or Egger's test  
âš ï¸ **Multiple Testing Correction** - Advanced stats tab exists but limited integration  
âš ï¸ **Sensitivity Analysis** - No systematic sensitivity/robustness testing framework  
âš ï¸ **Meta-Analysis Infrastructure** - Could benefit from basic meta-analytic capabilities  

---

## I. Current Architecture Deep Dive

### A. Tab Structure & Module Organization

```
Primary Tabs (7 main modules)
â”œâ”€â”€ ðŸ“ Data Management (tab_data.py)
â”‚   â””â”€â”€ Data upload, preview, variable classification
â”œâ”€â”€ ðŸ“‹ Table 1 & Matching (tab_baseline_matching.py)
â”‚   â”œâ”€â”€ Baseline characteristics table generation
â”‚   â”œâ”€â”€ Propensity Score Matching (PSM)
â”‚   â””â”€â”€ Matched cohort comparison
â”œâ”€â”€ ðŸ§ª Diagnostic Tests (tab_diag.py)
â”‚   â”œâ”€â”€ Sensitivity, Specificity, PPV, NPV
â”‚   â””â”€â”€ ROC curve visualization
â”œâ”€â”€ ðŸ“Š Regression Models (tab_logit.py) â­ PRIMARY REGRESSION HUB
â”‚   â”œâ”€â”€ Binary Logistic Regression (3 methods: Auto/Firth/Standard)
â”‚   â”œâ”€â”€ Poisson Regression (with offset support)
â”‚   â”œâ”€â”€ Linear Regression (OLS + Robust + Stepwise + Bootstrap)
â”‚   â””â”€â”€ Subgroup Analysis (interaction testing)
â”œâ”€â”€ ðŸ“ˆ Correlation & ICC (tab_corr.py)
â”‚   â”œâ”€â”€ Pearson/Spearman correlation matrices
â”‚   â””â”€â”€ Intraclass Correlation Coefficients
â”œâ”€â”€ â³ Survival Analysis (tab_survival.py)
â”‚   â”œâ”€â”€ Kaplan-Meier curves (stratified & log-rank tests)
â”‚   â”œâ”€â”€ Cox Proportional Hazards models
â”‚   â”œâ”€â”€ Time-varying covariates (TVC)
â”‚   â””â”€â”€ Landmark analysis
â””â”€â”€ âš™ï¸ Settings (tab_settings.py)
    â”œâ”€â”€ Advanced statistics configuration (MCC, VIF, CI methods)
    â””â”€â”€ UI theme/layout settings
```

### B. Current Advanced Features

#### 1. **Regression Models Tab (tab_logit.py)** - Most Complex Module
- **Binary Logistic Regression**: Auto-detection of perfect separation, Firth support
- **Poisson Regression**: Count data, offset support for rate calculations
- **Linear Regression**: OLS, Robust SE, stepwise selection (forward/backward/both), Bootstrap CIs
- **Subgroup Analysis**: Heterogeneity testing with interaction p-values

#### 2. **Missing Data Framework** (config.py + integrated across modules)
```python
"missing": {
    "strategy": "complete-case",
    "user_defined_values": [],  # -99, 999, etc.
    "treat_empty_as_missing": True,
    "report_missing": True,
    "report_threshold_pct": 50,
}
```
- Transparent reporting of excluded observations
- Missing data thresholds flagged during analysis
- Consistent implementation across all 7 modules

#### 3. **Advanced Statistics Module** (tab_advanced_stats.py)
```python
"stats": {
    "mcc_enable": True,
    "mcc_method": "fdr_bh",           # Bonferroni, Holm, FDR, Sidak
    "vif_enable": True,
    "vif_threshold": 10,
    "ci_method": "auto",              # Wald, Profile, Auto
}
```
- Multiple Comparison Correction (MCC)
- Variance Inflation Factor (VIF) for collinearity
- Confidence Interval method selection

#### 4. **Visualization Pipeline**
- Plotly-based forest plots with interactive features
- Kaplan-Meier survival curves with risk tables
- ROC curves with AUC confidence intervals
- Linear regression diagnostic plots (4-plot panel)

---

## II. Strategic Recommendations: New Modules for Publication Excellence

### Recommendation Overview

**Goal:** Add 3-4 new professional analytics modules suitable for high-impact medical journals without adding navigation complexity. Strategy: **Use subtabs within existing structure** rather than creating new primary tabs.

**Publication-Grade Statistical Modules Missing:**
1. **Meta-Analysis & Effect Size Synthesis** â† Most impactful
2. **Sensitivity Analysis & Robustness Testing** â† Critical for manuscripts
3. **Publication Bias Assessment** â† Increasingly required by journals
4. **Model Validation & Calibration** â† Essential for prediction models

---

## III. Detailed Implementation Roadmap

### OPTION A: Integrate into Regression Models Tab (RECOMMENDED)
**Why**: Regression tab already is the hub for complex analyses. Adding subtabs preserves UI clarity.

```
ðŸ“Š Regression Models (Current Structure)
â”œâ”€â”€ ðŸ“ˆ Binary Logistic Regression
â”‚   â”œâ”€â”€ Analysis Options (existing)
â”‚   â”œâ”€â”€ Forest Plots (existing)
â”‚   â”œâ”€â”€ Detailed Report (existing)
â”‚   â””â”€â”€ âœ¨ NEW: Model Validation & Diagnostics
â”‚       â”œâ”€â”€ Calibration plots (observed vs predicted)
â”‚       â”œâ”€â”€ ROC analysis with confidence regions
â”‚       â”œâ”€â”€ Bootstrap validation curves
â”‚       â””â”€â”€ Cross-validation results
â”œâ”€â”€ ðŸ“Š Poisson Regression (existing)
â”œâ”€â”€ ðŸ“ Linear Regression (existing)
â”œâ”€â”€ ðŸ—£ï¸ Subgroup Analysis (existing)
â”œâ”€â”€ ðŸ” SUBTAB: Sensitivity Analysis (NEW)
â”‚   â”œâ”€â”€ One-way sensitivity analysis
â”‚   â”œâ”€â”€ Two-way sensitivity/tornado plots
â”‚   â”œâ”€â”€ Threshold analysis
â”‚   â””â”€â”€ Robustness to unmeasured confounding
â”œâ”€â”€ ðŸ“š Reference (existing)
â””â”€â”€ (Settings remain separate)
```

**Implementation Details:**

#### **SUBTAB 1: Model Validation & Diagnostics** (Added to Logistic Regression)
```python
# File: tabs/tab_logit.py (extend existing)
# Add new navset_tab panel in "ðŸ“ˆ Binary Logistic Regression"

UI Components:
â”œâ”€â”€ Calibration Plot
â”‚   â”œâ”€â”€ Predicted vs Observed probabilities
â”‚   â”œâ”€â”€ Slope & intercept display
â”‚   â””â”€â”€ Hosmer-Lemeshow test statistic
â”œâ”€â”€ Discrimination Metrics
â”‚   â”œâ”€â”€ ROC AUC with 95% CI
â”‚   â”œâ”€â”€ C-statistic
â”‚   â””â”€â”€ Sensitivity/Specificity at optimal threshold
â”œâ”€â”€ Bootstrap Validation
â”‚   â”œâ”€â”€ Optimism-corrected statistics
â”‚   â”œâ”€â”€ Apparent vs Validated AUC
â”‚   â””â”€â”€ Sample size dependency analysis
â””â”€â”€ Cross-Validation (k-fold)
    â”œâ”€â”€ Fold-wise AUC distribution
    â”œâ”€â”€ Model stability assessment
    â””â”€â”€ Overfitting indicators
```

#### **SUBTAB 2: Sensitivity Analysis** (NEW Primary Subtab)
```python
# File: tabs/tab_sensitivity_analysis.py (NEW MODULE)

Core Features:
â”œâ”€â”€ Univariate Sensitivity Analysis
â”‚   â”œâ”€â”€ Vary effect size estimates by Â±% increments
â”‚   â”œâ”€â”€ Test significance thresholds
â”‚   â””â”€â”€ Visualize OR/HR/beta confidence at varying assumptions
â”œâ”€â”€ Tornado Diagram (Two-way Sensitivity)
â”‚   â”œâ”€â”€ Rank variables by impact on conclusion
â”‚   â”œâ”€â”€ Parameter range inputs
â”‚   â””â”€â”€ Color-coded significance crossing
â”œâ”€â”€ Threshold Analysis
â”‚   â”œâ”€â”€ At what parameter value does conclusion flip?
â”‚   â”œâ”€â”€ Probability of conclusion robustness
â”‚   â””â”€â”€ Sample size/power dependency
â””â”€â”€ Unmeasured Confounding (E-value approach)
    â”œâ”€â”€ Calculate E-values for point estimate & CI
    â”œâ”€â”€ Visual E-value comparisons
    â””â”€â”€ Causal interpretation guidance
```

#### **SUBTAB 3: Publication Bias Assessment** (NEW Primary Subtab)
```python
# File: tabs/tab_publication_bias.py (NEW MODULE)

Core Features:
â”œâ”€â”€ Funnel Plot Analysis
â”‚   â”œâ”€â”€ Standard funnel plots (SE vs Effect)
â”‚   â”œâ”€â”€ Contour-enhanced funnel plots
â”‚   â”œâ”€â”€ Visual bias assessment
â”‚   â””â”€â”€ Publication line interpretation
â”œâ”€â”€ Statistical Tests
â”‚   â”œâ”€â”€ Egger's regression test
â”‚   â”œâ”€â”€ Begg's rank correlation test
â”‚   â”œâ”€â”€ Harbord test (for binary outcomes)
â”‚   â””â”€â”€ Peters test (for rare events)
â”œâ”€â”€ Bias Adjustment Methods
â”‚   â”œâ”€â”€ Trim-and-fill method
â”‚   â”œâ”€â”€ Duval's sensitivity analysis
â”‚   â””â”€â”€ Adjusted effect estimates visualization
â””â”€â”€ Reporting Standards
    â”œâ”€â”€ PRISMA checklist guidance
    â”œâ”€â”€ Risk of bias summary
    â””â”€â”€ Evidence quality assessment (GRADE)
```

#### **SUBTAB 4: Meta-Analysis Essentials** (NEW Primary Subtab)
```python
# File: tabs/tab_meta_analysis.py (NEW MODULE)

Core Features:
â”œâ”€â”€ Effect Size Entry & Synthesis
â”‚   â”œâ”€â”€ Import study data (CSV with ES, SE, N)
â”‚   â”œâ”€â”€ Auto-convert from raw data (OR, HR, MD, SMD)
â”‚   â””â”€â”€ Summary effect calculation
â”œâ”€â”€ Forest Plot Generation
â”‚   â”œâ”€â”€ Fixed-effects & Random-effects models
â”‚   â”œâ”€â”€ Heterogeneity statistics (IÂ², Ï„Â², Q-test)
â”‚   â”œâ”€â”€ Publication-ready forest plots
â”‚   â””â”€â”€ Subgroup forest plots
â”œâ”€â”€ Heterogeneity Assessment
â”‚   â”œâ”€â”€ IÂ² interpretation guidance
â”‚   â”œâ”€â”€ Ï„Â² (tau-squared) visualization
â”‚   â”œâ”€â”€ Q-statistic forest (CMA-style)
â”‚   â””â”€â”€ Meta-regression for moderators
â””â”€â”€ Summary Statistics Export
    â”œâ”€â”€ Study-level data table
    â”œâ”€â”€ Meta-analysis table (NEJM format)
    â””â”€â”€ Publication-ready tables
```

---

## IV. Recommended Implementation Sequence

### Phase 1: Foundation (Week 1-2)
**Priority: Critical**

1. **Create New Module Files**
   ```bash
   tabs/tab_sensitivity_analysis.py       # Sensitivity analysis module
   tabs/tab_publication_bias.py          # Publication bias assessment
   utils/sensitivity_analysis_lib.py      # Computation backend
   utils/publication_bias_lib.py         # Funnel plot & tests backend
   utils/meta_analysis_lib.py            # Meta-analysis computations
   ```

2. **Update Main App** (app.py)
   ```python
   # Add imports
   from tabs import tab_sensitivity_analysis, tab_publication_bias
   
   # Update tab_logit.py to include new subtabs in navset_tab
   # (No new primary tabs - just extend existing structures)
   ```

### Phase 2: Sensitivity Analysis Implementation (Week 2-3)
**Priority: High**

```python
# tabs/tab_sensitivity_analysis.py structure

from shiny import module, reactive, render, ui
from utils.sensitivity_analysis_lib import (
    sensitivity_one_way,
    sensitivity_tornado,
    calculate_e_values,
)

@module.ui
def sensitivity_analysis_ui() -> ui.TagChild:
    """UI for sensitivity/robustness testing"""
    return ui.div(
        ui.h3("ðŸ”„ Sensitivity & Robustness Analysis"),
        ui.navset_card_pill(
            ui.nav_panel("One-Way Sensitivity", ...),
            ui.nav_panel("Tornado Diagram", ...),
            ui.nav_panel("E-Values (Unmeasured Confounding)", ...),
            ui.nav_panel("Guide & Interpretation", ...),
        ),
    )

@module.server
def sensitivity_analysis_server(input, output, session, analysis_result, df):
    """Compute sensitivity variations"""
    @reactive.Effect
    @reactive.event(input.btn_run_sensitivity)
    def _run_sensitivity():
        # Extract OR/HR/Beta and CI from analysis_result
        # Run variations
        # Generate plots (Plotly tornado, line plots)
        pass
```

### Phase 3: Publication Bias Assessment (Week 3-4)
**Priority: High**

```python
# tabs/tab_publication_bias.py structure

from utils.publication_bias_lib import (
    create_funnel_plot,
    egger_test,
    trim_and_fill,
)

@module.ui
def publication_bias_ui() -> ui.TagChild:
    return ui.div(
        ui.h3("ðŸ“Š Publication Bias Assessment"),
        ui.navset_card_pill(
            ui.nav_panel("Funnel Plot", ...),
            ui.nav_panel("Statistical Tests", ...),
            ui.nav_panel("Bias Correction", ...),
        ),
    )
```

### Phase 4: Model Validation (Week 4-5)
**Priority: Medium**

Extend `tab_logit.py`:
```python
# Add new nav_panel to existing Logistic Regression tab

ui.nav_panel(
    "âœ… Model Validation & Calibration",
    ui.navset_card_pill(
        ui.nav_panel("Calibration", ...),
        ui.nav_panel("Bootstrap Validation", ...),
        ui.nav_panel("Cross-Validation", ...),
    )
)
```

### Phase 5: Meta-Analysis (Week 5-6)
**Priority: Medium (Optional in Phase 1)**

Separate new primary tab or extend regression:
```python
# Option A: New tab in app.py
ui.nav_panel(
    "ðŸŒ Meta-Analysis",
    wrap_with_container(tab_meta_analysis.meta_analysis_ui("meta"))
)

# Option B: As subtab in Regression (better for UX)
# Add to tab_logit.py navset_tab
ui.nav_panel("ðŸŒ Meta-Analysis", ...)
```

---

## V. Technical Architecture: Integration Points

### A. Data Flow Architecture

```
Current Flow:
Data Upload â†’ Data Management (tab_data.py)
           â†“
       Shared State: df, var_meta, df_matched, is_matched
           â†“
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“           â†“          â†“          â†“          â†“
  Logit      Survival   Diagnostic   Corr    Baseline
  (with      Analysis    Tests       ICC    Matching
  Subgroup)

NEW Flow (Enhanced):
    â†“           â†“          â†“          â†“          â†“
  Logit      Survival   Diagnostic   Corr    Baseline
  
  â””â”€â”€ NEW: Sensitivity Analysis (input: logit_result)
  â””â”€â”€ NEW: Publication Bias (input: effect sizes from meta-dataset)
  â””â”€â”€ NEW: Model Validation (input: logit_result)
  â””â”€â”€ NEW: Meta-Analysis (input: imported study data)
```

### B. Module Dependencies

```python
# utils/sensitivity_analysis_lib.py dependencies
import pandas as pd
import numpy as np
from scipy.stats import norm
import plotly.graph_objects as go
from utils.formatting import format_or, format_p_value

# Core functions
def sensitivity_one_way(effect: float, se: float, ci_low: float, ci_high: float,
                       var_name: str, range_pct: list[float]) -> pd.DataFrame:
    """Vary effect by Â±X%, show significance threshold crossing"""
    
def sensitivity_tornado(effects: dict, var_names: list[str]) -> go.Figure:
    """Create tornado diagram ranking parameter sensitivity"""

def calculate_e_values(point_est: float, ci_low: float) -> dict:
    """E-value calculation for unmeasured confounding (VanderWeele & Ding)"""
```

### C. Configuration Integration

Add to `config.py`:
```python
"sensitivity": {
    "variation_increments": [0.1, 0.25, 0.5, 0.75, 1.0, 1.5, 2.0],  # Multipliers
    "ci_crossover_detection": True,
    "tornado_var_limit": 15,  # Max variables in tornado plot
},
"publication_bias": {
    "funnel_type": "contour_enhanced",  # 'standard', 'contour_enhanced'
    "egger_test_enabled": True,
    "trim_and_fill_enabled": True,
    "prisma_checklist": True,
},
"meta_analysis": {
    "random_effects_model": "DL",  # DL (DerSimonian-Laird), RE (restricted ML)
    "heterogeneity_measure": "I2",
    "forest_plot_style": "publication",
}
```

---

## VI. UI/UX Design Recommendations

### A. Navigation Strategy (AVOID Tab Explosion)

**âŒ DON'T DO:**
```
Primary Tabs: Data | Table1 | Diag | Logit | Corr | Survival | Settings
            | Sensitivity | Publication Bias | Meta-Analysis | Validation
            (10 tabs = overwhelming)
```

**âœ… DO (Current Approach):**
```
Primary Tabs: Data | Table1 | Diag | Logit | Corr | Survival | Settings
             
Within Logit tab (nested subtabs):
  â”œâ”€â”€ ðŸ“ˆ Binary Logistic Regression
  â”‚   â”œâ”€â”€ Analysis Options
  â”‚   â”œâ”€â”€ Forest Plots
  â”‚   â”œâ”€â”€ Detailed Report
  â”‚   â””â”€â”€ âœ¨ Model Validation & Calibration (NEW)
  â”œâ”€â”€ ðŸ“Š Poisson Regression
  â”œâ”€â”€ ðŸ“ Linear Regression
  â”œâ”€â”€ ðŸ—£ï¸ Subgroup Analysis
  â”œâ”€â”€ ðŸ” Sensitivity Analysis (NEW SUBTAB)
  â”œâ”€â”€ ðŸ“Š Publication Bias Assessment (NEW SUBTAB)
  â”œâ”€â”€ ðŸŒ Meta-Analysis (NEW SUBTAB) - Optional
  â””â”€â”€ â„¹ï¸ Reference
```

**Benefit**: 7 primary tabs remain consistent, but analytical depth increases 3-4x through strategic subtabs.

### B. Workflow Integration Examples

**Example Workflow 1: Logistic Regression â†’ Validation â†’ Sensitivity**
```
1. User runs Binary Logistic Regression
2. Reviews Forest Plot in "ðŸŒ³ Forest Plots" subtab
3. Clicks "âœ… Model Validation" to assess:
   - Calibration: How well does model predict?
   - Bootstrap validation: Is performance stable?
   - Cross-validation: Generalizable?
4. If satisfied with model, clicks "ðŸ”„ Sensitivity"
5. Tests robustness:
   - How much can effect change before conclusion flips?
   - E-values for unmeasured confounding
6. Downloads combined report (analysis + validation + sensitivity)
```

**Example Workflow 2: Multi-Study Effect Synthesis**
```
1. User imports data from 5 clinical trials (CSV)
2. Uses "ðŸŒ Meta-Analysis" subtab
3. Specifies effect size (OR, HR, MD)
4. Generates forest plot showing:
   - Individual study ORs with CIs
   - Heterogeneity statistics (IÂ², Q-test)
5. Checks "ðŸ“Š Publication Bias"
6. Views funnel plot + Egger's test result
7. If asymmetry detected, applies trim-and-fill
8. Exports PRISMA-formatted table for manuscript
```

### C. Visual Design Consistency

Keep design language consistent with existing modules:

**Color Palette** (from tabs/_styling.py):
```python
COLORS = {
    "primary": "#0066cc",
    "secondary": "#6c757d",
    "success": "#28a745",
    "danger": "#dc3545",
    "warning": "#ffc107",
    "info": "#17a2b8",
}
```

**Component Standards**:
- All result panels use same card layout (ui.card)
- Forest plots use Plotly with consistent styling
- Status indicators (âœ… Complete, â³ Running, âŒ Error)
- Download buttons with consistent naming

---

## VII. Implementation Details: Code Templates

### A. Sensitivity Analysis Module (Minimal Example)

```python
# utils/sensitivity_analysis_lib.py

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from scipy.stats import norm

def sensitivity_one_way(point_est: float, se: float, ci_low: float, ci_high: float,
                       significance_level: float = 0.05) -> pd.DataFrame:
    """
    One-way sensitivity: Vary effect size by multipliers, test significance crossing.
    
    Output: DataFrame with columns:
        - multiplier: effect size multiplier (0.5x, 1x, 2x, etc.)
        - adjusted_or: adjusted effect estimate
        - adjusted_ci_low, adjusted_ci_high: adjusted CI
        - crosses_null: boolean (CI crosses 1 for OR)
        - crosses_significance: boolean (p-value crosses 0.05)
    """
    multipliers = np.array([0.5, 0.75, 1.0, 1.25, 1.5, 2.0, 3.0])
    
    results = []
    for mult in multipliers:
        adj_est = point_est * mult
        adj_se = se * mult
        adj_ci_low = adj_est - (1.96 * adj_se)
        adj_ci_high = adj_est + (1.96 * adj_se)
        
        z_stat = adj_est / adj_se
        p_value = 2 * (1 - norm.cdf(abs(z_stat)))
        
        results.append({
            'multiplier': f'{mult}x',
            'adjusted_estimate': adj_est,
            'adjusted_ci_low': adj_ci_low,
            'adjusted_ci_high': adj_ci_high,
            'p_value': p_value,
            'ci_crosses_null': (adj_ci_low <= 0 <= adj_ci_high) or (adj_ci_low <= 1 <= adj_ci_high),
        })
    
    return pd.DataFrame(results)

def calculate_e_values(point_est: float, ci_lower: float) -> dict:
    """
    Calculate E-values for unmeasured confounding.
    Based on VanderWeele & Ding (2017).
    
    E-value represents the minimum strength of association an unmeasured
    confounder must have with both exposure and outcome to explain away the finding.
    """
    # For a ratio measure (OR, HR, RR)
    rr = point_est
    rr_ci = ci_lower
    
    e_value_point = rr + np.sqrt(rr * (rr - 1))
    e_value_ci = rr_ci + np.sqrt(rr_ci * (rr_ci - 1))
    
    return {
        'e_value_point_estimate': max(e_value_point, 1.0),
        'e_value_confidence_limit': max(e_value_ci, 1.0),
        'interpretation': (
            f"To explain away a {rr:.2f}x observed association, "
            f"an unmeasured confounder would need to be associated "
            f"with both exposure and outcome by at least {max(e_value_point, 1.0):.2f}x."
        ),
    }

def create_tornado_plot(effects: dict, var_names: list[str]) -> go.Figure:
    """
    Create tornado diagram showing parameter sensitivity ranking.
    Wider bars = parameter has larger impact on outcome.
    """
    # Implementation: Sort by effect magnitude, create horizontal bar chart
    pass
```

### B. Publication Bias Module (Minimal Example)

```python
# utils/publication_bias_lib.py

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from scipy.stats import spearmanr, linregress

def egger_test(effects: np.ndarray, standard_errors: np.ndarray) -> dict:
    """
    Egger's regression test for funnel plot asymmetry.
    
    Regression: Z-score = a + b*(1/SE)
    H0: b = 0 (no asymmetry)
    Significant p indicates publication bias.
    """
    z_scores = effects / standard_errors
    inv_se = 1.0 / standard_errors
    
    slope, intercept, r_value, p_value, std_err = linregress(inv_se, z_scores)
    
    return {
        'intercept': intercept,
        'slope': slope,
        'intercept_se': std_err,
        'p_value': p_value,
        'asymmetric': p_value < 0.05,
        'interpretation': (
            f"Egger's test p-value = {p_value:.4f}. "
            f"{'Asymmetry detected (publication bias possible).' if p_value < 0.05 else 'No asymmetry detected (good).'}"
        ),
    }

def create_funnel_plot(effects: np.ndarray, standard_errors: np.ndarray,
                      study_names: list[str], asymmetry_pvalue: float) -> go.Figure:
    """
    Create contour-enhanced funnel plot (Plotly).
    """
    fig = go.Figure()
    
    # Add pseudo-95% CI lines
    max_se = standard_errors.max()
    se_line = np.linspace(0, max_se, 100)
    z_crit = 1.96
    ci_lower = -z_crit * se_line
    ci_upper = z_crit * se_line
    
    fig.add_trace(go.Scatter(x=ci_upper, y=se_line, mode='lines', 
                             name='95% CI', line=dict(dash='dash')))
    fig.add_trace(go.Scatter(x=ci_lower, y=se_line, mode='lines',
                             line=dict(dash='dash'), showlegend=False))
    
    # Add studies
    colors = ['red' if e < 0 else 'blue' for e in effects]
    fig.add_trace(go.Scatter(x=effects, y=standard_errors, mode='markers+text',
                             text=study_names, marker=dict(color=colors, size=10),
                             name='Studies'))
    
    fig.update_layout(title=f'Funnel Plot (Egger p = {asymmetry_pvalue:.3f})',
                     xaxis_title='Effect Size',
                     yaxis_title='Standard Error')
    
    return fig
```

---

## VIII. Testing Strategy

### A. Unit Tests (for new functions)

```python
# tests/unit/test_sensitivity_analysis.py

import pytest
from utils.sensitivity_analysis_lib import (
    sensitivity_one_way,
    calculate_e_values,
)

def test_sensitivity_one_way():
    """Test sensitivity analysis computation"""
    result = sensitivity_one_way(
        point_est=2.0,  # OR = 2.0
        se=0.2,
        ci_low=1.6,
        ci_high=2.5,
    )
    assert len(result) == 7  # 7 multipliers
    assert 'adjusted_estimate' in result.columns

def test_e_values():
    """Test E-value calculation"""
    e_vals = calculate_e_values(point_est=2.0, ci_lower=1.5)
    assert e_vals['e_value_point_estimate'] >= 1.0
```

### B. Integration Tests (workflow level)

```python
# tests/integration/test_sensitivity_pipeline.py

def test_logit_to_sensitivity_workflow(sample_df):
    """Test workflow: run logit â†’ extract result â†’ run sensitivity"""
    
    # 1. Run logistic regression
    html_report, or_res, aor_res, _ = analyze_outcome(
        'outcome', sample_df, var_meta={}, method='auto'
    )
    
    # 2. Extract point estimate and CI
    effect_point = aor_res['treatment']['or']
    effect_ci = aor_res['treatment']['ci_low']
    
    # 3. Run sensitivity
    sens_result = sensitivity_one_way(
        point_est=effect_point,
        ci_low=effect_ci,
        ...
    )
    
    # Verify result structure
    assert len(sens_result) > 0
    assert 'adjusted_estimate' in sens_result.columns
```

---

## IX. Performance Optimization Considerations

### A. Computational Complexity

| Analysis | Time Complexity | Notes |
|----------|-----------------|-------|
| One-way Sensitivity | O(1) | ~7 variations, instant |
| Tornado Diagram | O(n log n) | Sort by effect magnitude |
| Bootstrap Validation | O(n Ã— k) | n=sample size, k=bootstrap samples (1000) |
| Egger's Test | O(n) | Linear regression on effects |
| Funnel Plot | O(n) | Plot generation |
| Meta-Analysis | O(mÂ²) | m=studies, compute heterogeneity matrix |

### B. Caching Strategy

```python
# In tab_sensitivity_analysis.py server

from functools import lru_cache

@lru_cache(maxsize=128)
def _cached_sensitivity_calc(effect_tuple, se, multipliers_tuple):
    """Cache sensitivity calculations for same parameters"""
    return sensitivity_one_way(effect=effect_tuple, se=se, ...)
```

### C. Memory Management

- Sensitivity/Publication Bias: Minimal memory (data in memory already)
- Meta-Analysis: Optimize for up to 100 studies (manageable)
- Implement cleanup after module switching (already in place)

---

## X. Quality Assurance Checklist

### Before Deployment

- [ ] **Unit Tests Pass**: All sensitivity, publication bias functions tested
- [ ] **Integration Tests Pass**: End-to-end workflows work
- [ ] **Performance**: Sensitivity analysis <500ms, Meta-analysis <2s
- [ ] **Documentation**: Docstrings, user guides for each module
- [ ] **UI/UX Review**: Consistent design, clear navigation
- [ ] **Error Handling**: Graceful failures with user-friendly messages
- [ ] **Logging**: Debug info logged appropriately
- [ ] **Code Coverage**: >80% for new modules
- [ ] **Missing Data**: Transparent reporting in new modules
- [ ] **Publication Readiness**: Output suitable for journal tables

### NEJM/Lancet/JAMA Compatibility

âœ… **Effect Size Reporting**: OR/HR/MD with exact CIs (not ranges)
âœ… **P-values**: Exact values displayed (or <0.001 / >0.999)
âœ… **Forest Plots**: Publication-ready (log scale for OR/HR)
âœ… **Tables**: APA formatting, clear row labels
âœ… **Methodological Rigor**: Sensitivity & validation built-in
âœ… **Missing Data**: Transparent CONSORT-style reporting

---

## XI. Strategic Value & Expected Impact

### Publication Enhancement

| Analysis Type | Impact | Journal Expectation |
|---------------|--------|-------------------|
| **Sensitivity** | Test robustness of conclusions | Standard (increasingly required) |
| **Validation** | Demonstrate model reliability | Prediction model submissions |
| **Publication Bias** | Meta-analysis credibility | Systematic reviews (required) |
| **E-Values** | Unmeasured confounding impact | High-impact journals (preferred) |

### Competitive Advantage

- **Stat-shiny** will differentiate from R Shiny medical apps by offering **complete analysis pipeline**: raw data â†’ analysis â†’ validation â†’ robustness â†’ publication
- Supports **manuscript preparation** from data import through publication-ready tables
- **No researcher** needs to jump between R/Python/Stata/SAS for publication-grade analyses

---

## XII. Roadmap Summary

### Quarter Q1 2026 (Now - March)

**Phase 1: Sensitivity Analysis** âœ¨ HIGHEST PRIORITY
- Implement one-way sensitivity
- Create tornado plots
- Add E-value calculations
- Integration with Logit tab
- Expected completion: Week 3-4

**Phase 2: Publication Bias** âœ¨ HIGH PRIORITY  
- Funnel plots (standard + contour)
- Egger's, Begg's tests
- Trim-and-fill corrections
- New subtab in Regression
- Expected completion: Week 4-5

### Quarter Q2 2026 (April - June)

**Phase 3: Model Validation** (Medium Priority)
- Calibration plots
- Bootstrap validation curves
- Cross-validation framework
- Add to Logit Regression tab

**Phase 4: Meta-Analysis** (Optional/Nice-to-Have)
- Study data import (CSV)
- Effect size synthesis
- Heterogeneity analysis
- Forest plot generation (study-level)

---

## XIII. Appendix: Code Structure Reference

```
Current Files to Modify/Extend:
â”œâ”€â”€ app.py
â”‚   â””â”€â”€ ADD: import tab_sensitivity_analysis, tab_publication_bias
â”‚   â””â”€â”€ MODIFY: Call new servers in server() function
â”œâ”€â”€ tabs/
â”‚   â”œâ”€â”€ tab_logit.py
â”‚   â”‚   â””â”€â”€ EXTEND: Add new nav_panels for validation, sensitivity, pub bias
â”‚   â”œâ”€â”€ tab_sensitivity_analysis.py (NEW)
â”‚   â”‚   â””â”€â”€ UI + Server for sensitivity analysis
â”‚   â”œâ”€â”€ tab_publication_bias.py (NEW)
â”‚   â”‚   â””â”€â”€ UI + Server for publication bias
â”‚   â”œâ”€â”€ tab_meta_analysis.py (NEW - Optional)
â”‚   â”‚   â””â”€â”€ UI + Server for meta-analysis
â”‚   â””â”€â”€ _common.py
â”‚       â””â”€â”€ ADD: Shared styling components for new modules
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ sensitivity_analysis_lib.py (NEW)
â”‚   â”‚   â”œâ”€â”€ sensitivity_one_way()
â”‚   â”‚   â”œâ”€â”€ calculate_e_values()
â”‚   â”‚   â”œâ”€â”€ create_tornado_plot()
â”‚   â”‚   â””â”€â”€ threshold_analysis()
â”‚   â”œâ”€â”€ publication_bias_lib.py (NEW)
â”‚   â”‚   â”œâ”€â”€ egger_test()
â”‚   â”‚   â”œâ”€â”€ begg_test()
â”‚   â”‚   â”œâ”€â”€ trim_and_fill()
â”‚   â”‚   â””â”€â”€ create_funnel_plot()
â”‚   â”œâ”€â”€ meta_analysis_lib.py (NEW - Optional)
â”‚   â”‚   â”œâ”€â”€ meta_analyze_binary()
â”‚   â”‚   â”œâ”€â”€ heterogeneity_stats()
â”‚   â”‚   â”œâ”€â”€ meta_regression()
â”‚   â”‚   â””â”€â”€ forest_plot_meta()
â”‚   â””â”€â”€ formatting.py
â”‚       â””â”€â”€ ADD: Format functions for new analysis outputs
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_sensitivity_analysis.py (NEW)
â”‚   â”‚   â””â”€â”€ test_publication_bias.py (NEW)
â”‚   â””â”€â”€ integration/
â”‚       â”œâ”€â”€ test_sensitivity_pipeline.py (NEW)
â”‚       â””â”€â”€ test_publication_bias_pipeline.py (NEW)
â”œâ”€â”€ config.py
â”‚   â””â”€â”€ ADD: New "sensitivity", "publication_bias", "meta_analysis" sections
â””â”€â”€ requirements.txt
    â””â”€â”€ ADD: New dependencies if needed (statsmodels for meta-analysis)
```

---

## XIV. Conclusion

**stat-shiny** has an excellent foundation. By adding these 3-4 modules through strategic subtabs rather than new primary tabs:

âœ… **Complete analysis pipeline**: Data â†’ Analysis â†’ Validation â†’ Sensitivity â†’ Publication  
âœ… **Publication excellence**: NEJM/Lancet/JAMA compatible  
âœ… **Navigation clarity**: 7 main tabs preserved, analytical depth multiplied  
âœ… **Professional standards**: Sensitivity, validation, publication bias assessment  
âœ… **Researcher efficiency**: No need to leave the platform for robustness testing  
âœ… **Competitive advantage**: Comprehensive medical statistics in a single, accessible web app  

**Estimated Total Implementation Time**: 6-8 weeks (2 weeks per module with testing & documentation)  
**Skill Requirements**: Python statistical libraries (scipy, statsmodels), Plotly visualization, Shiny reactive programming  
**Expected Impact**: Transform from "good analysis tool" to "gold-standard publication platform"

---

## References & Standards Alignment

ðŸ“š **Statistical Standards**:
- VanderWeele & Ding (2017): E-value framework for unmeasured confounding
- DerSimonian & Laird (1986): Meta-analysis random effects
- Egger et al. (1997): Funnel plot asymmetry detection
- Duval & Tweedie (2000): Trim-and-fill publication bias correction

ðŸ“Š **Publication Standards**:
- PRISMA 2020: Systematic review & meta-analysis reporting
- STROBE: Observational studies reporting
- CONSORT: Randomized trials reporting
- NEJM, Lancet, JAMA: Effect size and CI reporting conventions

---

**Report Prepared For**: NTWKKM stat-shiny development team  
**Analysis Date**: January 20, 2026  
**Status**: Ready for implementation planning