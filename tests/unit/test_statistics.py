"""
ðŸ§ª Comprehensive Unit Tests for Statistical Analysis Modules

Tests all core statistical functions across:
- logic.py: Logistic regression, data validation, outcome analysis
- diag_test.py: Diagnostic tests (ChiÂ², Fisher, ROC, Kappa, ICC)
- survival_lib.py: Survival analysis (KM, Cox, Nelson-Aalen, Landmark)

Run with: pytest tests/unit/test_statistics.py -v
"""

import pytest
import pandas as pd
import numpy as np
import sys
import os
from unittest.mock import MagicMock, patch
from typing import Any

# ============================================================================
# PATH SETUP & MOCKING
# ============================================================================
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

# Mock heavy dependencies BEFORE imports
sys.modules['plotly'] = MagicMock()
sys.modules['plotly.graph_objects'] = MagicMock()
sys.modules['plotly.express'] = MagicMock()
sys.modules['plotly.io'] = MagicMock()
sys.modules['forest_plot_lib'] = MagicMock()
sys.modules['lifelines'] = MagicMock()
sys.modules['lifelines.statistics'] = MagicMock()
sys.modules['lifelines.utils'] = MagicMock()
sys.modules['sklearn'] = MagicMock()
sys.modules['sklearn.metrics'] = MagicMock()
sys.modules['sklearn.linear_model'] = MagicMock()

# Import modules under test
from logic import (
    validate_logit_data, 
    clean_numeric_value, 
    run_binary_logit,
    analyze_outcome,
    get_label,
    fmt_p_with_styling
)

from diag_test import (
    calculate_descriptive, 
    calculate_chi2, 
    analyze_roc,
    calculate_kappa,
    calculate_icc,
    format_p_value,
    calculate_ci_wilson_score,
    auc_ci_delong
)

from survival_lib import (
    fit_km_logrank, 
    fit_cox_ph, 
    calculate_median_survival,
    fit_nelson_aalen,
    fit_km_landmark
)

# Mark all tests as unit tests
pytestmark = pytest.mark.unit


# ============================================================================
# SHARED FIXTURES
# ============================================================================

@pytest.fixture
def small_numeric_df():
    """Small dataset for quick validation tests."""
    return pd.DataFrame({
        'age': [20, 30, 40, 50],
        'outcome': [0, 1, 0, 1]
    })


@pytest.fixture
def dummy_df():
    """Comprehensive synthetic dataset for all test types."""
    np.random.seed(42)
    n = 200
    return pd.DataFrame({
        # Demographics
        'age': np.random.normal(50, 10, n),
        'sex': np.random.choice(['Male', 'Female'], n),
        
        # Binary variables
        'exposure': np.random.choice([0, 1], n),
        'disease': np.random.choice([0, 1], n, p=[0.7, 0.3]),
        'treatment': np.random.choice(['A', 'B'], n),
        
        # Continuous predictors
        'test_score': np.random.normal(0, 1, n) + np.random.choice([0, 1], n) * 2,
        'biomarker': np.random.exponential(2, n),
        
        # Survival data
        'time': np.random.exponential(10, n),
        'event': np.random.choice([0, 1], n, p=[0.6, 0.4]),
        
        # Rating data
        'rater1': np.random.choice([0, 1, 2], n),
        'rater2': np.random.choice([0, 1, 2], n)
    })


@pytest.fixture
def perfect_separation_df():
    """Dataset with perfect separation for Firth testing."""
    return pd.DataFrame({
        'predictor': [1, 1, 1, 1, 2, 2, 2, 2],
        'outcome': [0, 0, 0, 0, 1, 1, 1, 1]
    })


# ============================================================================
# LOGIC.PY TESTS - Logistic Regression & Data Validation
# ============================================================================

class TestDataValidation:
    """Tests for data quality checks and preprocessing."""
    
    def test_clean_numeric_value_basic(self):
        """âœ… Test numeric string parsing."""
        assert clean_numeric_value("1,000") == 1000.0
        assert clean_numeric_value("1000") == 1000.0
        assert clean_numeric_value(1000) == 1000.0
    
    def test_clean_numeric_value_comparison_symbols(self):
        """âœ… Test removal of comparison operators."""
        assert clean_numeric_value("<0.001") == 0.001
        assert clean_numeric_value("> 50") == 50.0
        assert clean_numeric_value("<=100") == 100.0
    
    def test_clean_numeric_value_invalid(self):
        """âœ… Test invalid input handling."""
        assert pd.isna(clean_numeric_value("abc"))
        assert pd.isna(clean_numeric_value(None))
        assert pd.isna(clean_numeric_value(""))
        assert pd.isna(clean_numeric_value("N/A"))
    
    def test_validate_logit_data_success(self, small_numeric_df):
        """âœ… Test validation with valid data."""
        y = small_numeric_df['outcome']
        X = small_numeric_df[['age']]
        valid, msg = validate_logit_data(y, X)
        assert valid is True
        assert msg == "OK"
    
    def test_validate_logit_data_zero_variance(self):
        """âœ… Test detection of constant variables."""
        df = pd.DataFrame({
            'constant': [1, 1, 1, 1],
            'age': [20, 30, 40, 50]
        })
        y = pd.Series([0, 1, 0, 1])
        valid, msg = validate_logit_data(y, df)
        assert valid is False
        assert "zero variance" in msg.lower()
    
    def test_validate_logit_data_empty(self):
        """âœ… Test empty data detection."""
        df = pd.DataFrame()
        y = pd.Series([])
        valid, msg = validate_logit_data(y, df)
        assert valid is False
        assert "empty" in msg.lower()
    
    def test_validate_logit_data_single_predictor(self):
        """âœ… Test single predictor validation."""
        y = pd.Series([0, 1, 0, 1, 0, 1])
        X = pd.DataFrame({'x': [1, 2, 3, 4, 5, 6]})
        valid, msg = validate_logit_data(y, X)
        assert valid is True
        assert msg == "OK"


class TestLogisticRegression:
    """Tests for binary logistic regression fitting."""
    
    def test_run_binary_logit_basic(self):
        """âœ… Test logistic regression with synthetic data."""
        np.random.seed(42)
        n = 100
        age = np.random.normal(50, 10, n)
        
        # Create clear relationship: older age â†’ higher probability of outcome=1
        prob = 1 / (1 + np.exp(-(0.1 * (age - 50))))
        y = pd.Series(np.random.binomial(1, prob))
        
        X = pd.DataFrame({'age': age})
        
        params, conf, pvals, status, metrics = run_binary_logit(y, X)
        
        # Verify successful fit
        assert status == "OK", f"Expected 'OK', got '{status}'"
        assert params is not None, "Parameters should not be None"
        assert 'age' in params.index, "Age coefficient should exist"
        
        # Age coefficient should be positive (matches synthetic pattern)
        assert params['age'] > 0, f"Expected positive age coefficient, got {params['age']}"
        
        # Model quality checks
        assert metrics['mcfadden'] > 0, "McFadden RÂ² should be > 0"
        assert 0 <= metrics['mcfadden'] <= 1, "McFadden RÂ² should be in [0,1]"
        
        # P-value should be significant
        assert pvals['age'] < 0.05, "Age should be statistically significant"
    
    def test_run_binary_logit_multiple_predictors(self):
        """âœ… Test multivariate logistic regression."""
        np.random.seed(42)
        n = 150
        age = np.random.normal(50, 10, n)
        weight = np.random.normal(70, 15, n)
        
        # Both predictors affect outcome
        logit = 0.05 * (age - 50) + 0.02 * (weight - 70) - 0.5
        prob = 1 / (1 + np.exp(-logit))
        y = pd.Series(np.random.binomial(1, prob))
        
        X = pd.DataFrame({'age': age, 'weight': weight})
        
        params, conf, pvals, status, metrics = run_binary_logit(y, X)
        
        assert status == "OK"
        assert 'age' in params.index
        assert 'weight' in params.index
        assert params['age'] > 0
        assert params['weight'] > 0
    
    def test_run_binary_logit_perfect_separation(self, perfect_separation_df):
        """âœ… Test handling of perfect separation."""
        y = perfect_separation_df['outcome']
        X = perfect_separation_df[['predictor']]
        
        # Default method should fail gracefully
        params, conf, pvals, status, metrics = run_binary_logit(y, X, method='default')
        
        # Should either succeed or return informative error
        if status != "OK":
            assert "separation" in status.lower() or "singular" in status.lower()
    
    def test_run_binary_logit_constant_outcome(self):
        """âœ… Test with constant outcome (no variation)."""
        y = pd.Series([1, 1, 1, 1, 1])
        X = pd.DataFrame({'x': [1, 2, 3, 4, 5]})
        
        params, conf, pvals, status, metrics = run_binary_logit(y, X)
        
        # Should fail due to lack of outcome variation
        assert status != "OK", "Should fail when outcome has no variation"


class TestFormattingHelpers:
    """Tests for formatting and display helper functions."""
    
    def test_fmt_p_with_styling_significant(self):
        """âœ… Test p-value formatting with styling."""
        result = fmt_p_with_styling(0.001)
        # Should contain significance indicator
        assert ("&lt;0.001" in result or "<0.001" in result)
        assert "sig-p" in result  # Should have significance class
    
    def test_fmt_p_with_styling_not_significant(self):
        """âœ… Test non-significant p-value formatting."""
        result = fmt_p_with_styling(0.234)
        assert "0.234" in result
        assert "sig-p" not in result  # Should NOT have significance class
    
    def test_fmt_p_with_styling_edge_cases(self):
        """âœ… Test edge case p-values."""
        assert "-" in fmt_p_with_styling(None)
        assert "-" in fmt_p_with_styling(np.nan)
        assert ">0.999" in fmt_p_with_styling(0.9999)
        assert ("&lt;0.001" in fmt_p_with_styling(0.0001) or "<0.001" in fmt_p_with_styling(0.0001))
    
    def test_get_label_basic(self):
        """âœ… Test label generation without metadata."""
        label = get_label("age", None)
        assert "age" in label
        assert "<b>" in label  # Should be bolded
    
    def test_get_label_with_metadata(self):
        """âœ… Test label generation with metadata."""
        var_meta = {'age': {'label': 'Patient Age (years)'}}
        label = get_label("age", var_meta)
        assert "age" in label
        assert "Patient Age" in label


class TestAnalyzeOutcome:
    """Tests for comprehensive outcome analysis."""
    
    def test_analyze_outcome_basic(self, dummy_df):
        """âœ… Test basic outcome analysis."""
        html, or_results, aor_results, int_results = analyze_outcome(
            'disease', 
            dummy_df[['age', 'exposure', 'disease']]
        )
        
        # Verify HTML output
        assert html is not None
        assert len(html) > 0
        assert 'disease' in html.lower()
        
        # Verify OR results
        assert isinstance(or_results, dict)
        assert isinstance(aor_results, dict)
    
    def test_analyze_outcome_missing_column(self, dummy_df):
        """âœ… Test with non-existent outcome."""
        html, or_results, aor_results, int_results = analyze_outcome(
            'nonexistent_outcome',
            dummy_df
        )
        
        assert 'not found' in html.lower() or 'error' in html.lower()
        assert len(or_results) == 0
        assert len(aor_results) == 0
    
    def test_analyze_outcome_non_binary(self, dummy_df):
        """âœ… Test with non-binary outcome."""
        df = dummy_df.copy()
        df['multi_outcome'] = np.random.choice([0, 1, 2], len(df))
        
        html, or_results, aor_results, int_results = analyze_outcome(
            'multi_outcome',
            df
        )
        
        # Should fail with informative message
        assert 'invalid' in html.lower() or 'expected 2' in html.lower()


# ============================================================================
# DIAG_TEST.PY TESTS - Diagnostic Statistics
# ============================================================================

class TestDescriptiveStatistics:
    """Tests for descriptive statistics calculations."""
    
    def test_calculate_descriptive_numeric(self, dummy_df):
        """âœ… Test descriptive stats for numeric variables."""
        result = calculate_descriptive(dummy_df, 'age')
        
        assert result is not None
        assert not result.empty
        assert isinstance(result, pd.DataFrame)
        
        # Check for expected statistics
        stats_present = result['Statistic'].values
        assert 'Mean' in stats_present
        assert 'SD' in stats_present
        assert 'Median' in stats_present
    
    def test_calculate_descriptive_categorical(self, dummy_df):
        """âœ… Test descriptive stats for categorical variables."""
        result = calculate_descriptive(dummy_df, 'sex')
        
        assert result is not None
        assert not result.empty
        assert isinstance(result, pd.DataFrame)
        
        # Should have Category and Count columns
        assert 'Category' in result.columns or 'Count' in result.columns
    
    def test_calculate_descriptive_missing_column(self, dummy_df):
        """âœ… Test with non-existent column."""
        result = calculate_descriptive(dummy_df, 'nonexistent')
        assert result is None
    
    def test_calculate_descriptive_empty_column(self):
        """âœ… Test with all-NA column."""
        df = pd.DataFrame({'empty_col': [np.nan, np.nan, np.nan]})
        result = calculate_descriptive(df, 'empty_col')
        assert result is None


class TestChiSquareAnalysis:
    """Tests for chi-square and contingency table analysis."""
    
    def test_calculate_chi2_basic(self, dummy_df):
        """âœ… Test basic chi-square calculation."""
        display_tab, stats_df, msg, risk_df = calculate_chi2(
            dummy_df, 'exposure', 'disease'
        )
        
        assert stats_df is not None
        assert isinstance(stats_df, pd.DataFrame)
        assert len(stats_df) > 0
        
        # Verify chi-square statistic is present
        test_row = stats_df[stats_df['Statistic'] == 'Test']
        assert not test_row.empty
        assert 'Chi-Square' in test_row['Value'].values[0]
    
    def test_calculate_chi2_2x2_risk_metrics(self, dummy_df):
        """âœ… Test 2x2 table risk metrics calculation."""
        display_tab, stats_df, msg, risk_df = calculate_chi2(
            dummy_df, 'exposure', 'disease'
        )
        
        # For 2x2 tables, risk metrics should be calculated
        if risk_df is not None:
            assert isinstance(risk_df, pd.DataFrame)
            
            # Check for expected metrics
            metrics = risk_df['Metric'].values
            assert any('Risk Ratio' in str(m) for m in metrics)
            assert any('Odds Ratio' in str(m) for m in metrics)
    
    def test_calculate_chi2_fisher_exact(self):
        """âœ… Test Fisher's Exact Test."""
        df = pd.DataFrame({
            'treatment': ['A', 'B'] * 10,
            'outcome': [0, 1, 0, 1, 0, 0, 1, 1, 0, 1] * 2
        })
        
        display_tab, stats_df, msg, risk_df = calculate_chi2(
            df, 'treatment', 'outcome', method='Fisher'
        )
        
        assert stats_df is not None
        test_row = stats_df[stats_df['Statistic'] == 'Test']
        assert 'Fisher' in test_row['Value'].values[0]
    
    def test_calculate_chi2_missing_columns(self, dummy_df):
        """âœ… Test with missing columns."""
        display_tab, stats_df, msg, risk_df = calculate_chi2(
            dummy_df, 'nonexistent1', 'nonexistent2'
        )
        
        assert stats_df is None
        assert msg is not None
        assert 'not found' in msg.lower()
    
    def test_calculate_chi2_empty_data(self):
        """âœ… Test with empty DataFrame."""
        df = pd.DataFrame({'col1': [], 'col2': []})
        display_tab, stats_df, msg, risk_df = calculate_chi2(
            df, 'col1', 'col2'
        )
        
        assert msg is not None
        assert 'no data' in msg.lower()


class TestROCAnalysis:
    """Tests for ROC curve and AUC calculation."""
    
    def test_analyze_roc_basic(self, dummy_df):
        """âœ… Test basic ROC analysis."""
        # Ensure we have sufficient variation
        n = len(dummy_df)
        dummy_df['disease_binary'] = np.random.choice([0, 1], n, p=[0.7, 0.3])
        dummy_df['score_values'] = np.random.uniform(0, 1, n)
        
        stats_dict, error_msg, fig, coords = analyze_roc(
            dummy_df, 'disease_binary', 'score_values', pos_label_user='1'
        )
        
        if stats_dict is not None:
            assert 'AUC' in stats_dict
            auc_val = float(stats_dict['AUC'])
            assert 0 <= auc_val <= 1, f"AUC must be in [0,1], got {auc_val}"
            
            # Verify CI is present
            assert '95% CI' in stats_dict
        else:
            assert error_msg is not None
    
    def test_analyze_roc_perfect_classifier(self):
        """âœ… Test ROC with perfect classifier (AUC=1.0)."""
        n = 100
        df = pd.DataFrame({
            'outcome': [0] * 50 + [1] * 50,
            'score': list(range(50)) + list(range(50, 100))  # Perfect separation
        })
        
        stats_dict, error_msg, fig, coords = analyze_roc(
            df, 'outcome', 'score', pos_label_user='1'
        )
        
        if stats_dict:
            auc_val = float(stats_dict['AUC'])
            assert auc_val > 0.95, f"Expected near-perfect AUC, got {auc_val}"
    
    def test_analyze_roc_missing_column(self, dummy_df):
        """âœ… Test ROC with missing column."""
        stats_dict, error_msg, fig, coords = analyze_roc(
            dummy_df, 'nonexistent', 'test_score', pos_label_user='1'
        )
        
        assert stats_dict is None
        assert error_msg is not None
    
    def test_analyze_roc_constant_score(self):
        """âœ… Test ROC with constant prediction score."""
        df = pd.DataFrame({
            'outcome': [0, 1, 0, 1, 0, 1],
            'score': [5.0, 5.0, 5.0, 5.0, 5.0, 5.0]  # All same
        })
        
        stats_dict, error_msg, fig, coords = analyze_roc(
            df, 'outcome', 'score', pos_label_user='1'
        )
        
        assert stats_dict is None
        assert error_msg is not None
        assert 'constant' in error_msg.lower()


class TestKappaAnalysis:
    """Tests for Cohen's Kappa inter-rater agreement."""
    
    def test_calculate_kappa_basic(self):
        """âœ… Test basic Kappa calculation."""
        # Create real data without mocks
        df = pd.DataFrame({
            'rater1': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1] * 5,
            'rater2': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1] * 5
        })
        
        res_df, error_msg, conf_matrix = calculate_kappa(
            df, 'rater1', 'rater2'
        )
        
        assert res_df is not None
        assert error_msg is None
        
        # Check for Kappa statistic
        assert "Cohen's Kappa" in res_df['Statistic'].values
        
        # Verify confusion matrix
        assert conf_matrix is not None
        assert isinstance(conf_matrix, pd.DataFrame)
    
    def test_calculate_kappa_perfect_agreement(self):
        """âœ… Test Kappa with perfect agreement."""
        df = pd.DataFrame({
            'rater1': [0, 1, 0, 1, 0, 1] * 10,
            'rater2': [0, 1, 0, 1, 0, 1] * 10  # Identical
        })
        
        res_df, error_msg, conf_matrix = calculate_kappa(df, 'rater1', 'rater2')
        
        assert res_df is not None
        kappa_row = res_df[res_df['Statistic'] == "Cohen's Kappa"]
        kappa_val = float(kappa_row['Value'].values[0])
        assert kappa_val > 0.95, f"Expected near-perfect Kappa, got {kappa_val}"
    
    def test_calculate_kappa_no_agreement(self):
        """âœ… Test Kappa with no agreement."""
        df = pd.DataFrame({
            'rater1': [0, 0, 0, 0, 0, 0],
            'rater2': [1, 1, 1, 1, 1, 1]  # Complete disagreement
        })
        
        res_df, error_msg, conf_matrix = calculate_kappa(df, 'rater1', 'rater2')
        
        assert res_df is not None
        kappa_row = res_df[res_df['Statistic'] == "Cohen's Kappa"]
        kappa_val = float(kappa_row['Value'].values[0])
        assert kappa_val < 0.1  # Should be very low


class TestICCAnalysis:
    """Tests for Intraclass Correlation Coefficient."""
    
    def test_calculate_icc_basic(self):
        """âœ… Test basic ICC calculation."""
        # Create correlated rater data
        np.random.seed(42)
        n = 50
        true_score = np.random.normal(50, 10, n)
        rater1 = true_score + np.random.normal(0, 2, n)
        rater2 = true_score + np.random.normal(0, 2, n)
        
        df = pd.DataFrame({'rater1': rater1, 'rater2': rater2})
        
        icc_df, error_msg, anova_df = calculate_icc(df, ['rater1', 'rater2'])
        
        assert icc_df is not None
        assert error_msg is None
        assert 'ICC' in icc_df.columns
        
        # Should return multiple ICC types
        assert len(icc_df) >= 2
    
    def test_calculate_icc_insufficient_columns(self):
        """âœ… Test ICC with single column."""
        df = pd.DataFrame({'rater1': [1, 2, 3, 4, 5]})
        
        icc_df, error_msg, anova_df = calculate_icc(df, ['rater1'])
        
        assert icc_df is None
        assert error_msg is not None


class TestConfidenceIntervals:
    """Tests for confidence interval calculations."""
    
    def test_wilson_score_ci(self):
        """âœ… Test Wilson score confidence intervals."""
        lower, upper = calculate_ci_wilson_score(30, 100)
        
        # Check bounds are valid
        assert 0 <= lower <= 1
        assert 0 <= upper <= 1
        assert lower < upper
        
        # Proportion should be within CI
        proportion = 30 / 100
        assert lower <= proportion <= upper
    
    def test_wilson_score_ci_edge_cases(self):
        """âœ… Test Wilson CI edge cases."""
        # Zero successes
        lower, upper = calculate_ci_wilson_score(0, 100)
        assert np.isclose(lower, 0, atol=1e-10)
        assert upper > 0
        
        # All successes
        lower, upper = calculate_ci_wilson_score(100, 100)
        assert np.isclose(upper, 1, atol=1e-10)
        assert lower < 1
    
    def test_delong_auc_ci(self):
        """âœ… Test DeLong AUC confidence intervals."""
        np.random.seed(42)
        y_true = np.array([0, 0, 0, 1, 1, 1])
        y_scores = np.array([0.1, 0.2, 0.3, 0.6, 0.7, 0.8])
        
        ci_lower, ci_upper, se = auc_ci_delong(y_true, y_scores)
        
        if not np.isnan(ci_lower):
            assert 0 <= ci_lower <= 1
            assert 0 <= ci_upper <= 1
            assert ci_lower < ci_upper


class TestPValueFormatting:
    """Tests for p-value formatting functions."""
    
    def test_format_p_value_significant(self):
        """âœ… Test significant p-value formatting."""
        result = format_p_value(0.001)
        assert "&lt;0.001" in result
        assert "font-weight: bold" in result
    
    def test_format_p_value_not_significant(self):
        """âœ… Test non-significant p-value."""
        result = format_p_value(0.234)
        assert "0.2340" in result
        assert "bold" not in result
    
    def test_format_p_value_edge_cases(self):
        """âœ… Test p-value edge cases."""
        assert "NA" in format_p_value(np.nan)
        assert "NA" in format_p_value(np.inf)


# ============================================================================
# SURVIVAL_LIB.PY TESTS - Survival Analysis
# ============================================================================

class TestMedianSurvival:
    """Tests for median survival time calculations."""
    
    def test_calculate_median_survival_basic(self, dummy_df):
        """âœ… Test median survival calculation."""
        result = calculate_median_survival(
            dummy_df, 'time', 'event', 'exposure'
        )
        
        assert not result.empty
        assert isinstance(result, pd.DataFrame)
        
        # Should have results for each group
        assert len(result) == 2  # Two exposure groups
        assert 'Median Time (95% CI)' in result.columns
    
    def test_calculate_median_survival_no_group(self, dummy_df):
        """âœ… Test median survival without grouping."""
        result = calculate_median_survival(
            dummy_df, 'time', 'event', None
        )
        
        assert not result.empty
        assert len(result) == 1  # Overall only
        assert 'Overall' in result['Group'].values
    
    def test_calculate_median_survival_missing_column(self, dummy_df):
        """âœ… Test with missing columns."""
        with pytest.raises(ValueError) as exc_info:
            calculate_median_survival(
                dummy_df, 'nonexistent_time', 'event', 'exposure'
            )
        assert 'missing' in str(exc_info.value).lower()


class TestKaplanMeier:
    """Tests for Kaplan-Meier survival curves."""
    
    def test_fit_km_logrank_basic(self, dummy_df):
        """âœ… Test KM curves with log-rank test."""
        # Ensure non-empty colors by ensuring groups exist
        dummy_df['group'] = dummy_df['exposure']
        
        fig, stats_df = fit_km_logrank(
            dummy_df, 'time', 'event', 'group'
        )
        
        assert fig is not None
        assert stats_df is not None
        assert isinstance(stats_df, pd.DataFrame)
        
        # Should contain P-value
        assert 'P-value' in stats_df.columns
        
        # P-value should be valid
        p_value = stats_df.iloc[0]['P-value']
        assert isinstance(p_value, (int, float))
        assert 0 <= p_value <= 1
    
    def test_fit_km_logrank_no_group(self, dummy_df):
        """âœ… Test KM without grouping variable."""
        fig, stats_df = fit_km_logrank(
            dummy_df, 'time', 'event', None
        )
        
        assert fig is not None
        # Stats should indicate single group
        assert 'Note' in stats_df.columns or len(stats_df) == 0


class TestNelsonAalen:
    """Tests for Nelson-Aalen cumulative hazard."""
    
    def test_fit_nelson_aalen_basic(self, dummy_df):
        """âœ… Test Nelson-Aalen estimator."""
        dummy_df['group'] = dummy_df['exposure']
        
        fig, stats_df = fit_nelson_aalen(
            dummy_df, 'time', 'event', 'group'
        )
        
        assert fig is not None
        assert stats_df is not None
        assert isinstance(stats_df, pd.DataFrame)
        
        # Should have group statistics
        assert 'N' in stats_df.columns
        assert 'Events' in stats_df.columns


class TestCoxRegression:
    """Tests for Cox Proportional Hazards model."""
    
    def test_fit_cox_ph_basic(self, dummy_df):
        """âœ… Test Cox regression."""
        cph, res_df, data, err = fit_cox_ph(
            dummy_df, 'time', 'event', ['age', 'exposure']
        )
        
        if cph is not None:
            assert res_df is not None
            assert isinstance(res_df, pd.DataFrame)
            
            # Check for expected covariates
            assert 'age' in res_df.index
            assert 'exposure' in res_df.index
            
            # Check for hazard ratios
            assert 'HR' in res_df.columns
        else:
            # If failed, should have error message
            assert err is not None
    
    def test_fit_cox_ph_no_events(self, dummy_df):
        """âœ… Test Cox with all censored data."""
        df = dummy_df.copy()
        df['event'] = 0  # All censored
        
        cph, res_df, data, err = fit_cox_ph(
            df, 'time', 'event', ['age', 'exposure']
        )
        
        assert cph is None
        assert err is not None
        assert 'no events' in err.lower() or 'censored' in err.lower()
    
    def test_fit_cox_ph_missing_columns(self, dummy_df):
        """âœ… Test Cox with missing columns."""
        cph, res_df, data, err = fit_cox_ph(
            dummy_df, 'nonexistent_time', 'event', ['age']
        )
        
        assert cph is None
        assert err is not None
        assert 'missing' in err.lower()


class TestLandmarkAnalysis:
    """Tests for landmark survival analysis."""
    
    def test_fit_km_landmark_basic(self, dummy_df):
        """âœ… Test landmark analysis."""
        landmark_time = 5.0
        dummy_df['group'] = dummy_df['exposure']
        
        fig, stats_df, n_pre, n_post, err = fit_km_landmark(
            dummy_df, 'time', 'event', 'group', landmark_time
        )
        
        if fig is not None:
            assert stats_df is not None
            assert n_pre >= n_post  # Some patients excluded
            assert n_post > 0
        else:
            assert err is not None
    
    def test_fit_km_landmark_insufficient_data(self, dummy_df):
        """âœ… Test landmark with time beyond all observations."""
        landmark_time = 1000.0  # Beyond max time
        
        fig, stats_df, n_pre, n_post, err = fit_km_landmark(
            dummy_df, 'time', 'event', 'exposure', landmark_time
        )
        
        assert fig is None
        assert err is not None
        assert 'insufficient' in err.lower()


# ============================================================================
# INTEGRATION & EDGE CASE TESTS
# ============================================================================

class TestIntegrationScenarios:
    """Tests for realistic analysis workflows."""
    
    def test_full_diagnostic_workflow(self, dummy_df):
        """âœ… Test complete diagnostic analysis pipeline."""
        # Step 1: Descriptive statistics
        desc = calculate_descriptive(dummy_df, 'age')
        assert desc is not None
        
        # Step 2: Chi-square test
        _, stats, _, risk = calculate_chi2(dummy_df, 'exposure', 'disease')
        assert stats is not None
        
        # Step 3: ROC analysis
        dummy_df_roc = dummy_df.copy()
        dummy_df_roc['disease_binary'] = dummy_df_roc['disease'].astype(int)
        dummy_df_roc['score_values'] = np.random.uniform(0, 1, len(dummy_df_roc))
        
        roc_stats, _, _, _ = analyze_roc(
            dummy_df_roc, 'disease_binary', 'score_values', pos_label_user='1'
        )
        # ROC may succeed or fail depending on data
        assert roc_stats is not None or _ is not None
    
    def test_full_survival_workflow(self, dummy_df):
        """âœ… Test complete survival analysis pipeline."""
        # Step 1: Median survival
        median_df = calculate_median_survival(
            dummy_df, 'time', 'event', 'exposure'
        )
        assert not median_df.empty
        
        # Step 2: KM curves
        dummy_df_km = dummy_df.copy()
        dummy_df_km['group'] = dummy_df_km['exposure']
        
        fig, stats = fit_km_logrank(dummy_df_km, 'time', 'event', 'group')
        assert fig is not None
        assert stats is not None
        
        # Step 3: Cox regression
        cph, res_df, _, err = fit_cox_ph(
            dummy_df, 'time', 'event', ['age', 'exposure']
        )
        # Cox may succeed or fail
        assert cph is not None or err is not None
    
    def test_full_logistic_workflow(self, dummy_df):
        """âœ… Test complete logistic regression pipeline."""
        # Step 1: Data validation
        y = dummy_df['disease']
        X = dummy_df[['age', 'exposure']]
        valid, msg = validate_logit_data(y, X)
        assert valid is True
        
        # Step 2: Univariate logistic
        params, conf, pvals, status, metrics = run_binary_logit(y, X)
        assert status == "OK"
        
        # Step 3: Full outcome analysis
        html, or_res, aor_res, int_res = analyze_outcome(
            'disease', 
            dummy_df[['age', 'exposure', 'disease']]
        )
        assert len(html) > 0


class TestEdgeCases:
    """Tests for boundary conditions and error handling."""
    
    def test_empty_dataframe_all_functions(self):
        """âœ… Test all functions with empty DataFrame."""
        df = pd.DataFrame()
        
        # Diagnostic tests
        assert calculate_descriptive(df, 'col') is None
        
        display, stats, msg, risk = calculate_chi2(df, 'c1', 'c2')
        assert stats is None or msg is not None
        
        # Survival tests
        with pytest.raises(ValueError):
            calculate_median_survival(df, 'time', 'event', 'group')
    
    def test_all_missing_data(self):
        """âœ… Test with all-NA data."""
        df = pd.DataFrame({
            'col1': [np.nan, np.nan, np.nan],
            'col2': [np.nan, np.nan, np.nan]
        })
        
        assert calculate_descriptive(df, 'col1') is None
    
    def test_single_observation(self):
        """âœ… Test with minimal data (n=1)."""
        df = pd.DataFrame({
            'outcome': [1],
            'predictor': [5.0]
        })
        
        # Should fail gracefully
        y = df['outcome']
        X = df[['predictor']]
        params, conf, pvals, status, metrics = run_binary_logit(y, X)
        assert status != "OK"


# ============================================================================
# PERFORMANCE & OPTIMIZATION TESTS
# ============================================================================

class TestPerformance:
    """Tests for computational efficiency (optional)."""
    
    @pytest.mark.slow
    def test_large_dataset_chi2(self):
        """âœ… Test chi-square with large dataset."""
        n = 10000
        df = pd.DataFrame({
            'var1': np.random.choice(['A', 'B'], n),
            'var2': np.random.choice([0, 1], n)
        })
        
        import time
        start = time.time()
        display, stats, msg, risk = calculate_chi2(df, 'var1', 'var2')
        elapsed = time.time() - start
        
        assert stats is not None
        assert elapsed < 5.0  # Should complete in <5 seconds
    
    @pytest.mark.slow
    def test_large_dataset_logistic(self):
        """âœ… Test logistic regression with large dataset."""
        np.random.seed(42)
        n = 5000
        X = pd.DataFrame({
            'x1': np.random.normal(0, 1, n),
            'x2': np.random.normal(0, 1, n)
        })
        logit = 0.5 * X['x1'] + 0.3 * X['x2']
        prob = 1 / (1 + np.exp(-logit))
        y = pd.Series(np.random.binomial(1, prob))
        
        import time
        start = time.time()
        params, conf, pvals, status, metrics = run_binary_logit(y, X)
        elapsed = time.time() - start
        
        assert status == "OK"
        assert elapsed < 10.0  # Should complete in <10 seconds


# ============================================================================
# MODULE IMPORT TEST
# ============================================================================

def test_module_imports():
    """âœ… Verify all modules imported successfully."""
    # If we got this far, imports succeeded
    assert validate_logit_data is not None
    assert calculate_chi2 is not None
    assert analyze_roc is not None
    assert fit_km_logrank is not None
    assert fit_cox_ph is not None
    assert calculate_median_survival is not None
    assert calculate_kappa is not None
    assert calculate_icc is not None
    assert clean_numeric_value is not None


# ============================================================================
# TEST SUITE SUMMARY
# ============================================================================

def test_suite_completeness():
    """âœ… Verify test suite covers all major functions."""
    tested_functions = [
        # logic.py
        'clean_numeric_value',
        'validate_logit_data', 
        'run_binary_logit',
        'analyze_outcome',
        'fmt_p_with_styling',
        'get_label',
        
        # diag_test.py
        'calculate_descriptive',
        'calculate_chi2',
        'analyze_roc',
        'calculate_kappa',
        'calculate_icc',
        'format_p_value',
        'calculate_ci_wilson_score',
        
        # survival_lib.py
        'calculate_median_survival',
        'fit_km_logrank',
        'fit_nelson_aalen',
        'fit_cox_ph',
        'fit_km_landmark'
    ]
    
    # All functions should be callable
    for func_name in tested_functions:
        assert func_name in dir(), f"Function {func_name} not imported"
    
    print(f"\nâœ… Test suite covers {len(tested_functions)} functions across 3 modules")
